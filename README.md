# ğŸ§  Deep Learning for NLP  
**March 2024 - April 2024**  

ğŸš€ **NLP | Deep Learning | Machine Translation**  

## ğŸ“Œ Project Overview  
This project explores **deep learning techniques for NLP tasks**, covering:  
- **CNN for Text Classification**: Used **convolutional neural networks (CNN)** to classify sentences.  
- **Sequence-to-Sequence Machine Translation**: Implemented **Seq2Seq models** and evaluated performance using **BLEU scores**.  
- **GPT-2 Fine-Tuning & Feature Extraction**: Fine-tuned **GPT-2** and compared **classification performance** with and without fine-tuning.  

## ğŸ—ï¸ Tech Stack  
âœ… **Languages & Libraries**: Python, TensorFlow, PyTorch, Scikit-learn, NLTK, Hugging Face Transformers  
âœ… **NLP Models**: CNN-based classifiers, Seq2Seq models, GPT-2  
âœ… **Evaluation Metrics**: BLEU score, Accuracy, AUROC, Precision, Recall  
âœ… **Data Processing**: Tokenization, GloVe embeddings, Data Augmentation  

## ğŸ“Š Key Highlights  
ğŸ“Œ **Built and evaluated CNN-based text classification models.**  
ğŸ“Œ **Implemented sequence-to-sequence models for machine translation.**  
ğŸ“Œ **Fine-tuned GPT-2 for language modeling and classification.**  

## ğŸ”§ Future Work  
ğŸ”œ Experimenting with **Transformer-based models** like T5 and BART.  
ğŸ”œ Improving BLEU scores using **attention-based models**.  

## ğŸš€ Running the Project  
1ï¸âƒ£ Open **Google Colab** or **Jupyter Notebook**.  
2ï¸âƒ£ Upload the following notebooks and run them in order:  
   - `CNN text classification.ipynb` â†’ for CNN-based Text Classification  
   - `Seq2Seq MT.ipynb` â†’ for Sequence-to-Sequence Machine Translation  
   - `GPT LM-3.ipynb` â†’ for GPT-2 Fine-Tuning & Evaluation  
   - `Experiments.ipynb` â†’ for Additional NLP Experiments  

